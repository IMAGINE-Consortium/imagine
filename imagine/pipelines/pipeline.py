# %% IMPORTS
# Built-in imports
import abc
from itertools import chain
import logging as log
import tempfile
import os
from os import path
import shutil
from collections import defaultdict

# Package imports
from astropy.table import QTable
import astropy.units as apu
from e13tools import q2tex
from mpi4py import MPI
import numpy as np
from scipy.stats import norm as scipy_norm
from scipy.stats import pearsonr as scipy_pearsonr
import IPython.display as ipd
import matplotlib.pyplot as plt
import hickle as hkl

# IMAGINE imports
from imagine import rc
from imagine.likelihoods import Likelihood
from imagine.fields import FieldFactory
from imagine.priors import Prior
from imagine.simulators import Simulator
from imagine.tools import BaseClass, ensemble_seed_generator, misc, visualization
from imagine.tools import io

# GLOBALS
comm = MPI.COMM_WORLD
mpisize = comm.Get_size()
mpirank = comm.Get_rank()

# All declaration
__all__ = ['Pipeline']


# %% CLASS DEFINITIONS
class Pipeline(BaseClass, metaclass=abc.ABCMeta):
    """
    Base class used for for initialing Bayesian analysis pipeline

    Attributes
    ----------
    likelihood_rescaler : double
        Rescale log-likelihood value
    random_type : str
        If set to 'fixed', the exact same set of ensemble seeds will be used
        for the evaluation of all fields, generated using the `master_seed`.
        If set to 'controllable', each individual field will get their own set
        of ensemble fields, but multiple runs will lead to the same results,
        as they are based on the same `master_seed`.
        If set to 'free', every time the pipeline is run, the `master_seed` is
        reset to a different value, and the ensemble seeds for each individual
        field are drawn based on this.
    master_seed : int
        Master seed used by the random number generators


    Parameters
    ----------
    simulator : imagine.simulators.simulator.Simulator
        Simulator object
    factory_list : list
        List or tuple of field factory objects
    likelihood : imagine.likelihoods.likelihood.Likelihood
        Likelihood object
    ensemble_size : int
        Number of observable realizations to be generated by the simulator
    run_directory : str
        Directory where the pipeline state and reports are saved.
    chains_directory : str
        Path of the directory where the chains should be saved. By default,
        this is saved to a 'chains' subdirectory of `run_directory`.
    prior_correlations : dict
        Dictionary used to set up prior distribution correlations. If two
        parameters are A and B are correlated a priori, an entry should be
        added to the prior_correlations dictionary in the form
        `(name_A, name_B): True`, to extract the correlation from the samples
        (in the case of CustomPriors) or `(name_A, name_B): value` otherwise.
    show_summary_reports : bool
        If True (default), shows/saves a corner plot and shows the evidence
        after the pipeline run has finished
    show_progress_reports : bool
        If True, shows/saves a simple progress of the sampler during the run.
    n_evals_report : int
        The number of likelihood evaluations before showing a progress report
    """

    def __init__(self, *, simulator, factory_list, likelihood, ensemble_size=1,
                 run_directory=None, chains_directory=None,
                 prior_correlations=None, show_summary_reports=True,
                 show_progress_reports=False, n_evals_report=500):
        # Call super constructor
        super().__init__()

        self.factory_list = factory_list
        # NB setting the factory list automatically sets: the active parameters,
        # parameter ranges and priors, based on the list
        self.simulator = simulator
        self.likelihood = likelihood
        self.prior_correlations = prior_correlations
        
        self.run_directory = run_directory
        self.chains_directory = chains_directory
        self.sampling_controllers = {}

        self._distribute_ensemble = rc['pipeline_distribute_ensemble']
        
        # Random seed settings
        self.random_type = 'controllable'
        # This may change on every execution if random_type=='free'
        self.master_seed = rc['pipeline_default_seed']
        # The ensemble_seeds are fixed in the case of the 'fixed' random_type;
        # or are regenerated on each Field evaluation, in the 'free' and
        # 'controllable' cases
        self.ensemble_seeds = None
        self.ensemble_size = ensemble_size

        # rescaling total likelihood in _core_likelihood
        self.likelihood_rescaler = 1
        # Checking likelihood threshold
        self.check_threshold = False
        self.likelihood_threshold = 0

        # Place holders
        self.sampler = None
        self.results = None
        self._evidence = None
        self._evidence_err = None
        self._posterior_summary = None
        self._samples_array = None
        self._samples = None
        self.correlated_priors = None
        self._median_simulation = None
        self._median_model = None

        # Report settings
        self.show_summary_reports = show_summary_reports
        self.show_progress_reports = show_progress_reports
        self.n_evals_report = n_evals_report
        self._likelihood_evaluations_counter = 0
        self.intermediate_results = defaultdict(lambda: None)
        

    def __call__(self, *args, save_pipeline_state=True, **kwargs):
        if save_pipeline_state:
            self.save()  # Keeps the setup safe
        result = self.call(*args, **kwargs)
        if self.show_summary_reports and (mpirank == 0):
            self.posterior_report()
            self.evidence_report()
        if save_pipeline_state:
            self.save()  # Keeps the results safe

        return result

    @property
    def run_directory(self):
        """
        Directory where the chains are stored
        (NB details of what is stored are sampler-dependent)
        """
        return self._run_directory

    @run_directory.setter
    def run_directory(self, run_directory):
        if run_directory is None:
            if mpirank == 0:
                # Creates a safe temporary directory in the current working directory
                self._run_dir_obj = tempfile.TemporaryDirectory(
                    prefix='imagine_run_', dir=rc['temp_dir'])
                # Note: this dir is automatically deleted together with the Pipeline object
                dir_path = self._run_dir_obj.name
            else:
                dir_path = None

            self._run_directory = comm.bcast(dir_path, root=0)
        else:
            # Removes previous temporary directory, if exists
            if hasattr(self, '_run_dir_obj'):
                del self._run_dir_obj
            # Creates new directory (if needed)
            os.makedirs(run_directory, exist_ok=True)
            assert path.isdir(run_directory)
            self._run_directory = run_directory

    @property
    def chains_directory(self):
        """
        Directory where the chains are stored
        (NB details of what is stored are sampler-dependent)
        """
        return self._chains_directory

    @chains_directory.setter
    def chains_directory(self, chains_directory):
        if chains_directory is None:
            chains_directory = os.path.join(self._run_directory, 'chains')
            os.makedirs(chains_directory, exist_ok=True)

        assert path.isdir(chains_directory)
        self._chains_directory = chains_directory


    def clean_chains_directory(self):
        """Removes the contents of the chains directory"""
        log.debug('@ pipeline::clean_chains_directory')

        if mpirank==0:
            for f in os.listdir(self._chains_directory):
                fullpath = path.join(self._chains_directory, f)
                try:
                    shutil.rmtree(fullpath)
                except NotADirectoryError:
                    os.remove(fullpath)
        comm.Barrier()

    @property
    def active_parameters(self):
        """
        List of all the active parameters
        """
        # The user should not be able to set this attribute manually
        return self._active_parameters

    @property
    def priors(self):
        """
        Dictionary containing priors for all active parameters
        """
        # The user should not be able to set this attribute manually
        return self._priors

    @property
    def posterior_summary(self):
        r"""
        A dictionary containing a summary of posterior statistics for each of
        the active parameters.  These are: 'median', 'errlo'
        (15.87th percentile), 'errup' (84.13th percentile), 'mean' and 'stdev'.
        """

        if self._posterior_summary is None:
            samp = self.samples

            self._posterior_summary = {}

            for name, column in zip(samp.columns, samp.itercols()):
                lo, median, up = np.percentile(column, [15.865, 50, 84.135])
                errlo = abs(median-lo)
                errup = abs(up-median)
                self._posterior_summary[name] = {
                    'median': median,
                    'errlo': errlo,
                    'errup': errup,
                    'mean': np.mean(column),
                    'stdev': np.std(column)}

        return self._posterior_summary

    @property
    def prior_correlations(self):
        return self._prior_correlations

    @prior_correlations.setter
    def prior_correlations(self, prior_correlations):
        if prior_correlations is None:
            # If None is provided, resets the correlations
            self._prior_correlations = None
            return
        else:
            # Otherwise, updates the prior correlations dictionary and
            # computes the L matrix

            # First, checks coefficient consistency
            correlated_priors = []
            new_prior_correlations = {}
            for n in list(prior_correlations.keys()):
                t = []
                for m in range(2):
                    for f in self._factory_list:
                        if list(n)[m] in f.active_parameters:
                            newname = f.name + '_' + list(n)[m]
                            correlated_priors.append(newname)
                            t.append(newname)
                new_prior_correlations.update({tuple(t): prior_correlations[n]})
            name_pairs = list(new_prior_correlations.keys())

            if len(name_pairs) != len(list(set(name_pairs))):
                raise ValueError('Inconsistent prior correlations, '
                                'possibly multiple values for the same coefficient')

            corr_matrix = np.eye(len(self._active_parameters))
            for name_pair in name_pairs:
                i, j = (self._prior_cube_mapping[param] for param in name_pair)
                c = new_prior_correlations[name_pair]
                if isinstance(c, bool):
                    if not c:
                        raise ValueError()

                    # Creates zero mean Gaussian distributions from the
                    # original samples
                    gaussian_pair = []
                    for name in name_pair:
                        prior = self._priors[name]
                        assert prior is not None

                        # Makes sure the samples are within the selected range
                        ok = (prior.samples >= prior.range[0])
                        ok *= (prior.samples <= prior.range[1])

                        x = scipy_norm.ppf(loc=0, scale=1,
                                           q=prior.cdf(prior.samples.value[ok]))

                        gaussian_pair.append(x)
                    # Computes the Pearson r correlation coefficient from the
                    # pair of distributions
                    c = scipy_pearsonr(*gaussian_pair)[0]
                else:
                    # If this was supplied, check whether the value is consistent
                    assert (-1 <= c <= 1)
                corr_matrix[i, j] = corr_matrix[j, i] = c

            # Computes the Cholesky L (lower-triangular) matrix which is used
            # to correlate the priors in the prior_transform method
            self._correlator_L = np.linalg.cholesky(corr_matrix)
            # Stores a list of correlated priors, for convenience
            self.correlated_priors = set(correlated_priors)
            # Finally, saves updated dictionary
            self._prior_correlations = new_prior_correlations

    def corner_plot(self, **kwargs):
        """
        Calls :py:func:`imagine.tools.visualization.corner_plot` to make
        a corner plot of samples produced by this Pipeline
        """
        return visualization.corner_plot(pipeline=self, **kwargs)


    def progress_report(self):
        """
        Reports the progress of the inference
        """
        self.get_intermediate_results()


        dead_samples = self.intermediate_results['rejected_points']
        live_samples = self.intermediate_results['live_points']
        likelihood = self.intermediate_results['logLikelihood']
        lnX = self.intermediate_results['lnX']
        if not ((dead_samples is None) or (likelihood is None) or (lnX is None)):
            fig = visualization.trace_plot(parameter_names=self._active_parameters,
                                  samples=dead_samples,
                                  live_samples=live_samples,
                                  likelihood=likelihood, lnX=lnX)
            fig_filepath = os.path.join(self._run_directory, 'progress_report.pdf')
            msg = 'Saving progress report to {}'.format(fig_filepath)
            log.info(msg)
            fig.savefig(fig_filepath)
            if misc.is_notebook():
                ipd.clear_output()
                ipd.display(ipd.Markdown("\n**Progress report:**"
                  '\nnumber of likelihood evaluations  {}'.format(
                    self._likelihood_evaluations_counter)))
                plt.show()
            else:
                print(msg)


    def posterior_report(self, sdigits=2, **kwargs):
        """
        Displays the best fit values and 1-sigma errors for each active
        parameter. Also produces a corner plot of the samples, which is
        saved to the run directory.

        If running on a jupyter-notebook, a nice LaTeX display is used, and
        the plot is shown.

        Parameters
        ----------
        sdigits : int
            The number of significant digits to be used
        """
        out = ''
        for param, pdict in self.posterior_summary.items():
            if misc.is_notebook():
                # Extracts LaTeX representation from astropy unit object
                out += r"\\ \text{{ {0}: }}\; ".format(param)
                out += q2tex(*map(pdict.get, ['median', 'errup', 'errlo']),
                             sdigits=sdigits)
                out += r"\\"
            else:
                out += r"{0}: ".format(param)
                md, errlo, errup = map(pdict.get, ['median', 'errlo', 'errup'])
                if isinstance(md, apu.Quantity):
                    unit = str(md.unit)
                    md, errlo, errup = map(lambda x: x.value, [md, errlo, errup])
                else:
                    unit = ""
                v, l, u = misc.adjust_error_intervals(
                    md, errlo, errup, sdigits=sdigits)
                out += r'{0} (-{1})/(+{2}) {3}\n'.format(v, l, u, unit)

        fig = self.corner_plot(**kwargs)
        fig.savefig(os.path.join(self._run_directory, 'corner_plot.pdf'))
        if misc.is_notebook():
            ipd.display(ipd.Markdown("\n**Posterior report:**"))
            plt.show()
            ipd.display(ipd.Math(out))
        else:
            # Restores linebreaks and prints
            print('Posterior report')
            print(out.replace(r'\n','\n'))


    def evidence_report(self, sdigits=4):
        if misc.is_notebook():
            ipd.display(ipd.Markdown("**Evidence report:**"))
            out = r"\log\mathcal{{ Z }} = "
            out += q2tex(self.log_evidence, self.log_evidence_err)
            ipd.display(ipd.Math(out))
        else:
            print('Evidence report')
            print('logZ =', self.log_evidence, '±', self.log_evidence_err)


    @property
    def log_evidence(self):
        r"""
        Natural logarithm of the *marginal likelihood* or *Bayesian model evidence*,
        :math:`\ln\mathcal{Z}`, where

        .. math::
            \mathcal{Z} = P(d|m) = \int_{\Omega_\theta} P(d | \theta, m) P(\theta | m) \mathrm{d}\theta .

        Note
        ----
        Available only after the pipeline is run.
        """
        if self._evidence is None:
            raise ValueError('Evidence not set! Have you run the pipeline?')
        else:
            return self._evidence

    @property
    def log_evidence_err(self):
        """
        Error estimate in the natural logarithm of the *Bayesian model evidence*.
        Available once the pipeline is run.

        Note
        ----
        Available only after the pipeline is run.
        """
        assert self._evidence_err is not None, 'Evidence error not set! Did you run the pipeline?'

        return self._evidence_err

    @property
    def median_model(self):
        """
        List of Field objects corresponding to the median values of the
        distributions of parameter values found after a Pipeline run.
        """
        if self._median_model is None:
            self._median_model = []
            for factory in self.factory_list:
                params_dict = {}
                for param, results_dict in self.posterior_summary.items():
                    if factory.field_name in param:
                        param = param.replace(factory.field_name + '_','')
                        params_dict[param] = results_dict['median']

                field = factory(ensemble_seeds=self.ensemble_seeds[factory],
                                variables=params_dict)
                self._median_model.append(field)
        return self._median_model

    @property
    def median_simulation(self):
        """
        Simulation corresponding to the
        :py:data:`median_model <Pipeline.median_model>`.
        """
        if self._median_simulation is None:
            self._median_simulation = self.simulator(self.median_model)
        return self._median_simulation

    @property
    def samples(self):
        """
        An :py:class:`astropy.table.QTable` object containing parameter values
        of the samples produced in the run.
        """
        if self._samples is None:
            assert self._samples_array is not None, 'Samples not available. Did you run the pipeline?'

            self._samples = QTable(data=self._samples_array,
                                   names=self._active_parameters)
            # Restores the units
            for param, prior in self._priors.items():
                self._samples[param] = self._samples[param] <<  prior.unit

        return self._samples

    @property
    def factory_list(self):
        """
        List of the
        :py:class:`Field Factories <imagine.fields.field_factory.GeneralFieldFactory>`
        currently being used.

        Updating the factory list automatically extracts active_parameters,
        parameter ranges and priors from each field factory.
        """
        return self._factory_list

    @factory_list.setter
    def factory_list(self, factory_list):
        # Notice that the parameter/variable ordering is fixed wrt
        # factory ordering. This is useful for recovering variable logic value
        # for each factory and necessary to construct the common prior function.
        assert isinstance(factory_list, (list, tuple)), 'Factory list must be a tuple or list'
        self._active_parameters = tuple()
        self._priors = dict()
        self._prior_cube_mapping = dict()

        i = 0

        for factory in factory_list:
            assert isinstance(factory, FieldFactory)
            for ap_name in factory.active_parameters:
                if ap_name in self._prior_cube_mapping:
                    raise KeyError('Ambiguous prior naming')
                self._prior_cube_mapping.update({factory.name+'_'+ap_name: i})
                assert isinstance(ap_name, str)
                # Sets the parameters and ranges
                self._active_parameters += (str(factory.name+'_'+ap_name),)
                # Sets the Prior
                prior = factory.priors[ap_name]
                assert isinstance(prior, Prior)
                self._priors[factory.name+'_'+ap_name] = prior
                i += 1
        self._factory_list = factory_list

    @property
    def sampler_supports_mpi(self):
        return getattr(self, 'SUPPORTS_MPI', False)

    @property
    def simulator(self):
        """
        The :py:class:`Simulator <imagine.simulators.simulator.Simulator>`
        object used by the pipeline
        """
        return self._simulator

    @simulator.setter
    def simulator(self, simulator):
        assert isinstance(simulator, Simulator)
        self._simulator = simulator

    @property
    def likelihood(self):
        """
        The :py:class:`Likelihood <imagine.likelihoods.likelihood.Likelihood>`
        object used by the pipeline
        """
        return self._likelihood

    @likelihood.setter
    def likelihood(self, likelihood):
        assert isinstance(likelihood, Likelihood)
        self._likelihood = likelihood

    def prior_pdf(self, cube):
        """
        Probability distribution associated with the all parameters being used by
        the multiple Field Factories

        Parameters
        ----------
        cube : np.ndarray
            Each row of the array corresponds to a different parameter value
            in the sampling (dimensionless, but in the standard units of the
            prior).

        Returns
        -------
        cube_rtn
            The modified cube
        """
        cube_rtn = np.empty_like(cube)
        for i, parameter in enumerate(self._active_parameters):
            prior = self._priors[parameter]
            cube_rtn[i] = prior.pdf(cube[i]*prior.unit)
        return cube_rtn

    def prior_transform(self, cube):
        """
        Prior transform cube

        Takes a cube containing a uniform sampling of  values and maps then onto
        a distribution compatible with the priors specified in the
        Field Factories.

        If prior correlations were specified, these are applied to the cube
        (assumes the correlations can be well described by the correlations
        between gaussians).

        Parameters
        ----------
        cube : array
            Each row of the array corresponds to a different parameter in the sampling.

        Returns
        -------
        cube
            The modified cube
        """
        cube_copy = cube.copy()

        if self.prior_correlations is not None:
            # Correlates the cube, using the previously computed Cholesky L matrix
            cube_copy = scipy_norm.cdf( self._correlator_L @ scipy_norm.ppf(cube_copy) )

        for i, parameter in enumerate(self._active_parameters):
            val = self._priors[parameter](cube_copy[i])
            if isinstance(val, apu.Quantity):
                val = val.value
            cube_copy[i] = val
        return cube_copy

    @property
    def distribute_ensemble(self):
        """
        If True, whenever the sampler requires a likelihood evaluation,
        the ensemble of stochastic fields realizations is distributed among
        all the nodes.

        Otherwise, each likelihood evaluations will go through the whole
        ensemble size on a single node. See :doc:`parallel` for details.
        """
        return self._distribute_ensemble

    @distribute_ensemble.setter
    def distribute_ensemble(self, distr_ensemble):
        # Saves the choice
        self._distribute_ensemble = distr_ensemble
        # Calls the setter method to ajust hidden parts
        self.ensemble_size = self.ensemble_size


    @property
    def ensemble_size(self):
        return self._ensemble_size

    @ensemble_size.setter
    def ensemble_size(self, ensemble_size):
        ensemble_size = int(ensemble_size)
        assert (ensemble_size > 0)
        self._ensemble_size = ensemble_size
        log.debug('set ensemble size to %i' % int(ensemble_size))
        
        if self._distribute_ensemble:
            # Sets pointer to the correct likelihood function
            self._likelihood_function = self._mpi_likelihood
            # Sets effective ensemble size
            if self.ensemble_size % mpisize != 0:
                raise ValueError("In 'distribute_ensemble' mode, ensemble_size "
                                 "must be a multiple of the number of MPI nodes")
            self.ensemble_size_actual = self.ensemble_size // mpisize
        else:
            # Sets pointer to the correct likelihood function
            self._likelihood_function = self._core_likelihood
            # Sets effective ensemble size
            self.ensemble_size_actual = self.ensemble_size
        self._randomness()
        

    @property
    def sampling_controllers(self):
        """
        Settings used by the sampler (e.g. `'dlogz'`).
        See the documentation of each specific pipeline subclass for details.

        After the pipeline runs, this property is updated to reflect the
        actual final choice of sampling controllers (including
        default values).
        """
        return self._sampling_controllers

    @sampling_controllers.setter
    def sampling_controllers(self, pp_dict):
        try:
            self._sampling_controllers.update(pp_dict)
        except AttributeError:
            self._sampling_controllers = pp_dict

    def tidy_up(self):
        """
        Resets internal state before a new run
        """
        log.debug('@ pipeline::tidy_up')

        self.results = None
        self._evidence = None
        self._evidence_err = None
        self._posterior_summary = None
        self._samples_array = None
        self._samples = None
        self._median_model = None
        self._median_simulation = None
        self._randomness()
        self._likelihood_evaluations_counter = 0
        self.intermediate_results = defaultdict(lambda: None)


    def _randomness(self):
        """
        Manipulate random seed(s)
        isolating this process for convenience of testing
        """
        log.debug('@ pipeline::_randomness')

        assert self.random_type in ('free', 'controllable', 'fixed')

        if self.random_type == 'free':
            # Refreshes the master seed
            self.master_seed = np.random.randint(0, 2**32)

        # Updates numpy random accordingly
        np.random.seed(self.master_seed)

        if self.random_type == 'fixed':
            common_ensemble_seeds = ensemble_seed_generator(self.ensemble_size_actual)
            self.ensemble_seeds = {factory: common_ensemble_seeds
                                   for factory in self._factory_list}
        elif self.random_type == 'controllable':
            self.ensemble_seeds = {factory: ensemble_seed_generator(self.ensemble_size_actual)
                                   for factory in self._factory_list}
        else:
            self.ensemble_seeds = {factory: None for factory in self._factory_list}

    # This function returns all parameter names of all factories in order
    def get_par_names(self):
        # Create list of names
        names = list(chain(*[factory.active_parameters
                             for factory in self._factory_list]))

        # Return them
        return(names)

    def _get_observables(self, cube):
        # return active variables from pymultinest cube to factories
        # and then generate new field objects
        head_idx = 0
        tail_idx = 0
        field_list = tuple()

        # the ordering in factory list and variable list is vital
        for factory in self._factory_list:
            variable_dict = dict()
            tail_idx = head_idx + len(factory.active_parameters)
            factory_cube = cube[head_idx:tail_idx]
            for i, av in enumerate(factory.active_parameters):
                variable_dict[av] = factory_cube[i]*self._priors[factory.field_name + '_' + av].unit

            ensemble_seeds = self.ensemble_seeds[factory]
            field_list += (factory(variables=variable_dict,
                                   ensemble_size=self.ensemble_size_actual,
                                   ensemble_seeds=ensemble_seeds),)
            log.debug('create '+factory.name+' field')
            head_idx = tail_idx
        assert(head_idx == len(self._active_parameters))

        observables = self._simulator(field_list)

        return(observables)

    def _core_likelihood(self, cube):
        """
        core log-likelihood calculator

        Parameters
        ----------
        cube
            list of variable values

        Returns
        -------
        log-likelihood value
        """
        log.debug('@ pipeline::_core_likelihood')
        log.debug('sampler at %s' % str(cube))

        # Obtain observables for provided cube
        observables = self._get_observables(cube)

        # add up individual log-likelihood terms
        current_likelihood = self.likelihood(observables)
        # check likelihood value until negative (or no larger than given threshold)
        if self.check_threshold and current_likelihood > self.likelihood_threshold:
            raise ValueError('log-likelihood beyond threshold')

        # Logs the value
        log.info('Likelihood evaluation at point:'
                 ' {0} value: {1}'.format(cube, current_likelihood))

        # Reports, if needed
        self._likelihood_evaluations_counter += 1
        if (self.show_progress_reports and
            self._likelihood_evaluations_counter % self.n_evals_report == 0):
            if mpirank==0:
                self.progress_report()

        return current_likelihood * self.likelihood_rescaler

    def _mpi_likelihood(self, cube):
        """
        mpi log-likelihood calculator
        PyMultinest supports execution with MPI
        where sampler on each node follows DIFFERENT journeys in parameter space
        but keep in communication
        so we need to firstly register parameter position on each node
        and calculate log-likelihood value of each node with joint force of all nodes
        in this way, ensemble size is multiplied by the number of working nodes

        Parameters
        ----------
        cube
            list of variable values

        Returns
        -------
        log-likelihood value
        """

        if self.sampler_supports_mpi:

            log.debug('@ pipeline::_mpi_likelihood')

            # Gathers cubes from all nodes
            cube_local_size = cube.size
            cube_pool = np.empty(cube_local_size*mpisize, dtype=np.float64)
            comm.Allgather([cube, MPI.DOUBLE], [cube_pool, MPI.DOUBLE])

            # Calculates log-likelihood for each node
            loglike_pool = np.empty(mpisize, dtype=np.float64)
            for i in range(mpisize):  # loop through nodes
                cube_local = cube_pool[i*cube_local_size : (i+1)*cube_local_size]

                loglike_pool[i] = self._core_likelihood(cube_local)

            # Scatters log-likelihood to each node
            loglike_local = np.empty(1, dtype=np.float64)
            comm.Scatter([loglike_pool, MPI.DOUBLE], [loglike_local, MPI.DOUBLE], root=0)

            return loglike_local[0]  # Some samplers require a scalar value

        else:

            log.debug('@ dynesty_pipeline::_mpi_likelihood')
            # gather cubes from all nodes
            cube_local_size = cube.size
            cube_pool = np.empty(cube_local_size*mpisize, dtype=np.float64)
            comm.Allgather([cube, MPI.DOUBLE], [cube_pool, MPI.DOUBLE])
            # check if all nodes are at the same parameter-space position
            assert ((cube_pool == np.tile(cube_pool[:cube_local_size], mpisize)).all())
            return self._core_likelihood(cube)

    @abc.abstractmethod
    def get_intermediate_results(self):
        raise NotImplementedError

    @abc.abstractmethod
    def call(self, **kwargs):
        raise NotImplementedError

    def save(self, **kwargs):
        io.save_pipeline(self, **kwargs)

    def __del__(self):
        # This MPI barrier ensures that all the processes reached the
        # same point before deleting the temporary directories
        comm.Barrier()
